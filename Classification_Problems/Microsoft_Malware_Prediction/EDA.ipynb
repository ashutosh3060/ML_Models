{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNeed to install imblearn & lightgbm\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic numerical and other libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import sys\n",
    "import functools\n",
    "\n",
    "\n",
    "# Display option\n",
    "from IPython.display import display, HTML\n",
    "pd.options.display.max_rows = 5000\n",
    "pd.options.display.max_columns = 5000\n",
    "# pd.set_option('display.max_columns', 5000)\n",
    "\n",
    "\n",
    "# Handle warnings (during execution of code)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Datetime\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# Visulisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "# Imbalanced Data Handling\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# statsmodel\n",
    "import statsmodels.api as statsm\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.metrics import confusion_matrix, auc, roc_curve, roc_auc_score, log_loss, mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, LeaveOneOut, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Advanced Model\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Save Model\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "'''\n",
    "Need to install imblearn & lightgbm\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 109.2MB 452kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.2MB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/04/4e36c33f8eb5c5b6c622a1f4859352a6acca7ab387257d4b3c191d23ec1d/gast-0.3.2.tar.gz\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 39.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/9b/ba5d094f979325fdba696bafa9ee23cc50b8fc60481e3d2a9e13d76817dc/grpcio-1.26.0-cp36-cp36m-manylinux1_x86_64.whl (2.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.5MB 15.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 18.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 55.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K    100% |████████████████████████████████| 491kB 40.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 36.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting keras-applications>=1.0.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 34.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Collecting setuptools>=41.0.0 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/d3/955738b20d3832dfa3cd3d9b07e29a8162edb480bf988332f5e6e48ca444/setuptools-44.0.0-py2.py3-none-any.whl (583kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 40.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 46.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Building wheels for collected packages: wrapt, termcolor, gast, absl-py\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/59/38/c6/234dc39b4f6951a0768fbc02d5b7207137a5b1d9094f0d54bf\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "Successfully built wrapt termcolor gast absl-py\n",
      "Installing collected packages: numpy, wrapt, termcolor, gast, google-pasta, astor, grpcio, setuptools, markdown, absl-py, tensorboard, tensorflow-estimator, keras-preprocessing, keras-applications, tensorflow\n",
      "  Found existing installation: numpy 1.14.3\n",
      "    Uninstalling numpy-1.14.3:\n",
      "      Successfully uninstalled numpy-1.14.3\n",
      "  Found existing installation: wrapt 1.10.11\n",
      "\u001b[31mCannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ef3825d26b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MultiOutputMixin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5824c9a186e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Handle imbalanced-class problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mModule\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mallowing\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcreate\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mscikit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_sampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/imblearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_docstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubstitution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_neighbors_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_sampling_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ball_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kd_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiOutputMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_distances_chunked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MultiOutputMixin'"
     ]
    }
   ],
   "source": [
    "# Handle imbalanced-class problem\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.13.2)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: xgboost in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.90)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.1.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.14.3)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: lightgbm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (0.20.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (1.14.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (1.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting imblearn\n",
      "  Downloading https://files.pythonhosted.org/packages/81/a7/4179e6ebfd654bd0eac0b9c06125b8b4c96a9d0a8ff9e9507eb2a26d2d7e/imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/aa/eba717a14df36f0b6f000ebfaf24c3189cd7987130f66cc3513efead8c2a/imbalanced_learn-0.6.1-py3-none-any.whl (162kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 15.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn>=0.22 (from imbalanced-learn->imblearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/48/e9fa9e252abcd1447eff6f9257636af31758a6e46fd5ce5d3c879f6907cb/scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.1MB 6.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11 (from imbalanced-learn->imblearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 55.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.14.3)\n",
      "Installing collected packages: joblib, scikit-learn, imbalanced-learn, imblearn\n",
      "  Found existing installation: scikit-learn 0.20.3\n",
      "    Uninstalling scikit-learn-0.20.3:\n",
      "      Successfully uninstalled scikit-learn-0.20.3\n",
      "Successfully installed imbalanced-learn-0.6.1 imblearn-0.0 joblib-0.14.1 scikit-learn-0.22.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n",
    "!pip install xgboost\n",
    "!pip install lightgbm\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute null and infinite values\n",
    "\n",
    "def treat_inf(dataset):\n",
    "    dataset.replace({np.inf:0,-np.inf:0,'inf':0,float('inf'):0},inplace = True)\n",
    "    return dataset\n",
    "\n",
    "# outlier_headers is present inside the data_preprocessing_functions\n",
    "def treatna(dataset,type=0):\n",
    "    if type ==0:\n",
    "        dataset.fillna(0,inplace= True)\n",
    "    elif type == 'mean':\n",
    "        inf_df = infer_schema(dataset,ret_inference= True)\n",
    "        continuos_values = inf_df[inf_df.number_of_classes>11].feature_name.values\n",
    "        dataset[continuos_values] = dataset[continuos_values].fillna(dataset[continuos_values].mean())\n",
    "    elif type == 'mode':\n",
    "        inf_df = infer_schema(dataset,ret_inference= True)\n",
    "        categorical = inf_df[inf_df.number_of_classes<=11].feature_name.values        \n",
    "        for col in categorical:\n",
    "            model_dataset[col] = model_dataset[col].fillna(model_dataset[col].value_counts().index[0])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Replace Outlier\n",
    "\n",
    "def replace_outlier(i, uv, lv):\n",
    "    if i < lv:\n",
    "        i = lv\n",
    "    elif i > uv:\n",
    "        i = uv\n",
    "    else:\n",
    "        i\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df.rename(columns={\"old_column1\": \"new_column1\", \"old_column2\": \"new_column2\"}, inplace=True)\n",
    "\n",
    "# Drop columns\n",
    "df.drop(columns=['column1', 'column2'])\n",
    "''' or the below option'''\n",
    "df.drop(['column1', 'column2'], axis = 1, inplace = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Encoding\n",
    "\n",
    "# LabelEncoding\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv('household_data.txt') \n",
    "X = df.iloc[:, :-1].values \n",
    "y = df.iloc[:, -1].values \n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:,0] = labelencoder_X.fit_transform(X[:,0])\n",
    "X[:,1] = labelencoder_X.fit_transform(X[:,1])\n",
    "print(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "\n",
    "# By avergae of a particular class for that class imputation only (This will help us imputing in a better way than imputing by taking average of the whole dataset)\n",
    "# function\n",
    "def impute_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    \n",
    "    if pd.isnull(Age):\n",
    "\n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "\n",
    "        elif Pclass == 2:\n",
    "            \n",
    "            return 29\n",
    "\n",
    "        else:\n",
    "            return 24\n",
    "\n",
    "    else:\n",
    "        return Age\n",
    "    \n",
    "# Apply the function to the Age column\n",
    "train_data['Age']=train_data[['Age','Pclass']].apply(impute_age, axis =1 ) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Treatment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "# x_dev = scaler.transform(x_dev)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalaced Data Handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle EDA\n",
    "https://www.kaggle.com/kredy10/basic-eda-and-classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summary\n",
    "df.describe() '''or the below'''\n",
    "df.describe().transpose()\n",
    "\n",
    "# Data types\n",
    "print(train_data.info())\n",
    "\n",
    "# Identify Categorical Features (below command will help in selecting columns having datatype 'object')\n",
    "print(train_data.select_dtypes(['object']).columns)\n",
    "\n",
    "\n",
    "# Target Variable class distribution\n",
    "print(df.groupby(['name of target_var']).target_var.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age by Passenger Class (2 Variables are analysed against each other)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.boxplot(x = 'Pclass', y = 'Age', data = train_data, palette= 'GnBu_d').set_title('Age by Passenger Class')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Target Variable Countplot\n",
    "fig, axes = plt.subplots(figsize=(10, 3))\n",
    "ncount = len(df)\n",
    "for i, ax in enumerate(fig.axes):\n",
    "        ax.set_xticklabels(ax.xaxis.get_majorticklabels(),rotation=90)\n",
    "        sns.countplot(x=df['df'], alpha=0.7, data=df, ax=ax)\n",
    "        for p in ax.patches:\n",
    "            x=p.get_bbox().get_points()[:,0]\n",
    "            y=p.get_bbox().get_points()[1,1]\n",
    "            ax.annotate('{:.2f}%'.format(100.*y/ncount), (x.mean(), y), ha='center', va='bottom')\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "# Identify numeric features (Continuous & Discrete features) (Below is titanic Dataset examples)\n",
    "print('Continuous Variables') \n",
    "print(train_data[['Age','Fare']].describe().transpose())\n",
    "print('--'*40)\n",
    "print('Discrete Variables')\n",
    "print(train_data.groupby('Pclass').Pclass.count())\n",
    "print(train_data.groupby('SibSp').SibSp.count())\n",
    "print(train_data.groupby('Parch').Parch.count())\n",
    "\n",
    "# Subplots of Numeric Features\n",
    "sns.set_style('darkgrid')\n",
    "fig = plt.figure(figsize = (20,16))\n",
    "fig.subplots_adjust(hspace = .30)\n",
    "\n",
    "ax1 = fig.add_subplot(321)\n",
    "ax1.hist(train_data['Pclass'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n",
    "ax1.set_xlabel('Pclass', fontsize = 15)\n",
    "ax1.set_ylabel('# Passengers',fontsize = 15)\n",
    "ax1.set_title('Passenger Class',fontsize = 15)\n",
    "\n",
    "ax2 = fig.add_subplot(323)\n",
    "ax2.hist(train_data['Age'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n",
    "ax2.set_xlabel('Age',fontsize = 15)\n",
    "ax2.set_ylabel('# Passengers',fontsize = 15)\n",
    "ax2.set_title('Age of Passengers',fontsize = 15)\n",
    "\n",
    "ax3 = fig.add_subplot(325)\n",
    "ax3.hist(train_data['SibSp'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n",
    "ax3.set_xlabel('SibSp',fontsize = 15)\n",
    "ax3.set_ylabel('# Passengers',fontsize = 15)\n",
    "ax3.set_title('Passengers with Spouses or Siblings',fontsize = 15)\n",
    "\n",
    "ax4 = fig.add_subplot(222)\n",
    "ax4.hist(train_data['Parch'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n",
    "ax4.set_xlabel('Parch',fontsize = 15)\n",
    "ax4.set_ylabel('# Passengers',fontsize = 15)\n",
    "ax4.set_title('Passengers with Children',fontsize = 15)\n",
    "\n",
    "ax5 = fig.add_subplot(224)\n",
    "ax5.hist(train_data['Fare'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n",
    "ax5.set_xlabel('Fare',fontsize = 15)\n",
    "ax5.set_ylabel('# Passengers',fontsize = 15)\n",
    "ax5.set_title('Ticket Fare',fontsize = 15)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Target vs Categorical Features\n",
    "print('Final Rating Summary')\n",
    "print('\\n')\n",
    "# Passenger class visualization\n",
    "print(funded_Q4_2017.groupby(['final_rating','df']).final_rating.count().unstack())\n",
    "final_rating = funded_Q4_2017.groupby(['final_rating','df']).final_rating.count().unstack()\n",
    "p1 = final_rating.plot(kind = 'bar', stacked = True, \n",
    "                   title = 'Applications by risk_band: defaulters vs Non-defaulters', \n",
    "                   color = ['grey','lightgreen'], alpha = .70)\n",
    "p1.set_xlabel('Final Rating')\n",
    "p1.set_ylabel('# Applications')\n",
    "p1.legend(['Non-Defaulter','Defaulter'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Feature1 vs Feature2\n",
    "# titanic.hist(bins=10,figsize=(9,7),grid=False)\n",
    "# Statistical summary of continuous variables \n",
    "print('Statistical Summary of Age and Fare')\n",
    "print('\\n')\n",
    "print('Did Not Survive')\n",
    "print(train_data[train_data['Survived']==0][['Age','Fare']].describe().transpose())\n",
    "print('--'*40)\n",
    "print('Survived')\n",
    "print(train_data[train_data['Survived']==1][['Age','Fare']].describe().transpose())\n",
    "\n",
    "\n",
    "# Subplots of Numeric Features\n",
    "sns.set_style('darkgrid')\n",
    "fig = plt.figure(figsize = (16,10))\n",
    "fig.subplots_adjust(hspace = .30)\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.hist(df_variable_data[df_variable_data['df'] ==0].PREMIER_V1_2_ALL8552, bins = 25, label ='Non-Defaulter', alpha = .50, edgecolor= 'black', color ='grey')\n",
    "ax1.hist(df_variable_data[df_variable_data['df']==1].PREMIER_V1_2_ALL8552, bins = 25, label = 'Defaulter', alpha = .50, edgecolor = 'black',color = 'lightgreen')\n",
    "ax1.set_title('PREMIER_V1_2_ALL8552: Defaulter vs Non-Defaulter')\n",
    "ax1.set_xlabel('PREMIER_V1_2_ALL8552')\n",
    "ax1.set_ylabel('# Applications')\n",
    "ax1.legend(loc = 'upper right')\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(223)\n",
    "ax2.hist(df_variable_data[df_variable_data['df']==0].VANTAGE_V3_SCORE, bins = 25, label = 'Non-Defaulter', alpha = .50, edgecolor ='black', color = 'grey')\n",
    "ax2.hist(df_variable_data[df_variable_data['df']==1].VANTAGE_V3_SCORE, bins = 25, label = 'Defaulter', alpha = .50, edgecolor = 'black',color ='lightgreen')\n",
    "ax2.set_title('VANTAGE_V3_SCORE: Defaulter vs Non-Defaulter')\n",
    "ax2.set_xlabel('VANTAGE_V3_SCORE')\n",
    "ax2.set_ylabel('# Applications')\n",
    "ax2.legend(loc = 'upper right')\n",
    "\n",
    "ax3 = fig.add_subplot(122)\n",
    "ax3.scatter(x = df_variable_data[df_variable_data['df']==0].PREMIER_V1_2_ALL8552, y = df_variable_data[df_variable_data['df']==0].VANTAGE_V3_SCORE,\n",
    "                        alpha = .50,edgecolor= 'black',  c = 'grey', s= 75, label = 'Did Not Survive')\n",
    "ax3.scatter(x = df_variable_data[df_variable_data['df']==1].PREMIER_V1_2_ALL8552, y = df_variable_data[df_variable_data['df']==1].VANTAGE_V3_SCORE,\n",
    "                        alpha = .50,edgecolors= 'black',  c = 'lightgreen', s= 75, label = 'Survived')\n",
    "ax3.set_xlabel('PREMIER_V1_2_ALL8552')\n",
    "ax3.set_ylabel('VANTAGE_V3_SCORE')\n",
    "ax3.set_title('PREMIER_V1_2_ALL8552 vs VANTAGE_V3_SCORE')\n",
    "ax3.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the statistics of a dataframe:\n",
    "df.describe() # only the numerical ones\n",
    "\n",
    "# Make correlation table\n",
    "corr = df.corr()\n",
    "corr\n",
    "\n",
    "# Visualisation of heatmap matrix\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate \n",
    "'''\n",
    "enumerate() method adds a counter to an iterable and returns it in a form of enumerate object. This enumerate object can then be used directly in for loops or be converted into a list of tuples using list() method\n",
    "# https://www.geeksforgeeks.org/enumerate-in-python/\n",
    "'''\n",
    "lst = ['low','medium','high']\n",
    "\n",
    "for i, sal in enumerate(lst):\n",
    "    df.salary.replace(to_replace=sal,value=i,inplace=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set=['TrueIpLatitude',\n",
    "'diff_domainAge',\n",
    "'EAScore',\n",
    "'PolicyScore',\n",
    "'TrueIpLongitude',\n",
    "'FraudScore',\n",
    "'inventories',\n",
    "'max_pg_cc',\n",
    "'dl95_currbalance_m6',\n",
    "'enq_90days',\n",
    "'amount_pounds',\n",
    "'discdeprcharges',\n",
    "'dirs_avg_age',\n",
    "'onclaccruals',\n",
    "'totalcurrassets',\n",
    "'IdentityIp',\n",
    "'totalcurrlblts',\n",
    "'dl95_numactiveaccs_m2',\n",
    "'nr_pg',\n",
    "'TrueIpScore',\n",
    "'othcurrlblts',\n",
    "'enq_own_90days',\n",
    "'TrueIpWorstScore',\n",
    "'full_time_employees',\n",
    "'max_nr_mortgage',\n",
    "'amount_requested',\n",
    "'avg_nr_mnt_elect_roll_curr_addr',\n",
    "'max_nr_mnt_elect_roll_curr_addr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prepartion for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create independent and target variable set\n",
    "x = df[feature_set]\n",
    "y = df[target_variable]\n",
    "\n",
    "# Dataset Prepartion for train and test\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)\n",
    "x_dev,x_test,y_dev,y_test = train_test_split(x_test,y_test,test_size=0.5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = H1_2019_amt_req_ltet50k\n",
    "target = H1_2019_amt_req_ltet50k['sphonic_rej_sf']\n",
    "test = H2_2018_amt_req_ltet50k\n",
    "\n",
    "# Insample train and validation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[feature_set], train['sphonic_rej_sf'], test_size=0.33, random_state=42)\n",
    "\n",
    "# Data for independent and target variables\n",
    "X_insample   = train[feature_set]\n",
    "X_oot        = test[feature_set]\n",
    "insample_act = train['sphonic_rej_sf']\n",
    "oot_act      = test['sphonic_rej_sf']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_split=3,)\n",
    "clf.fit(x_train,y_train)\n",
    "pred = clf.predict(x_train)\n",
    "print('Training Report\\n {}'.format(classification_report(y_train,pred)))\n",
    "print('Training accuracy: {:.3f}'.format(accuracy_score(y_train,pred)))\n",
    "\n",
    "\n",
    "pred = clf.predict(x_dev)\n",
    "print('Dev set Report\\n {}'.format(classification_report(y_dev,pred)))\n",
    "print('Dev set accuracy: {:.3f}'.format(accuracy_score(y_dev,pred)))\n",
    "print('AUC: {:.3f}'.format(roc_auc_score(y_dev,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=300)\n",
    "clf.fit(x_train,y_train)\n",
    "pred = clf.predict(x_train)\n",
    "print('Training Report\\n {}'.format(classification_report(y_train,pred)))\n",
    "print('Training accuracy: {:.3f}'.format(accuracy_score(y_train,pred)))\n",
    "\n",
    "pred = clf.predict(x_dev)\n",
    "print('Dev set Report\\n {}'.format(classification_report(y_dev,pred)))\n",
    "print('Dev set accuracy: {:.3f}'.format(accuracy_score(y_dev,pred)))\n",
    "print('AUC: {:.3f}'.format(roc_auc_score(y_dev,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classifier\n",
    "\n",
    "clf = SVC(C=500)\n",
    "clf.fit(x_train,y_train)\n",
    "pred = clf.predict(x_train)\n",
    "print('Training Report\\n {}'.format(classification_report(y_train,pred)))\n",
    "print('Training accuracy: {:.3f}'.format(accuracy_score(y_train,pred)))\n",
    "\n",
    "pred = clf.predict(x_dev)\n",
    "print('Dev set Report\\n {}'.format(classification_report(y_dev,pred)))\n",
    "print('Dev set accuracy: {:.3f}'.format(accuracy_score(y_dev,pred)))\n",
    "print('AUC: {:.3f}'.format(roc_auc_score(y_dev,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k Nearest Neighbors\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "training_start = time.perf_counter()\n",
    "knn.fit(X_train, y_train)\n",
    "training_end = time.perf_counter()\n",
    "prediction_start = time.perf_counter()\n",
    "preds = knn.predict(X_test)\n",
    "prediction_end = time.perf_counter()\n",
    "acc_knn = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "knn_train_time = training_end-training_start\n",
    "knn_prediction_time = prediction_end-prediction_start\n",
    "print(\"Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: %3.2f\" % (acc_knn))\n",
    "print(\"Time consumed for training: %4.3f seconds\" % (knn_train_time))\n",
    "print(\"Time consumed for prediction: %6.5f seconds\" % (knn_prediction_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['KNN', 'Naive Bayes', \n",
    "              'XGBoost', 'Random Forest', 'SVC'],\n",
    "    'Score': [acc_knn, acc_gnb, acc_xgb, acc_rfc, acc_svc],\n",
    "    'Runtime Training': [knn_train_time, gnb_train_time, xgb_train_time, rfc_train_time, \n",
    "                         svc_train_time],\n",
    "    'Runtime Prediction': [knn_prediction_time, gnb_prediction_time, xgb_prediction_time, rfc_prediction_time,\n",
    "                          svc_prediction_time]})\n",
    "result_df = results.sort_values(by='Score', ascending=False)\n",
    "result_df = result_df.set_index('Model')\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tips: Before feeding the data into the model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sample_learning_rate_option=[0.005,0.001,0.05,0.01,0.1]\n",
    "sample_leaf_options = [100,200,350,500]\n",
    "sample_max_depths = [3,5,8,10,20,50]\n",
    "subsample_list=[0.5,0.6,0.7,0.8,0.85]\n",
    "\n",
    "Hyperparameter_search_lightgbm =pd.DataFrame(columns=['subsample' ,'max_depth','learning Rate','leaf_size','log_loss','gini_insample','gini_oot'])\n",
    "  \n",
    "for subsample in subsample_list :\n",
    "    for max_depth in sample_max_depths :\n",
    "            for num_leaves in sample_leaf_options:\n",
    "                for lr in sample_learning_rate_option :\n",
    "                    train_data=lgb.Dataset(X_train,label=y_train)\n",
    "                    validtn_data=lgb.Dataset(X_test,label=y_test)\n",
    "                    param = {'num_leaves':num_leaves, 'objective':'binary','max_depth':max_depth, 'learning_rate': lr, 'subsample': subsample ,'boosting_type' : 'dart'}\n",
    "                    param['metric'] = ['auc', 'binary_logloss']\n",
    "                    start=datetime.now()\n",
    "                    print(\"model fit start\")\n",
    "                    lgbm_model=lgb.train(param,train_data,valid_sets=validtn_data,verbose_eval= 10)\n",
    "#                   lgbm_model_cv_results = lgb.cv(param,train_data,verbose_eval= 10,nfold=5,stratified=True)\n",
    "                    insample_pred = lgbm_model.predict(X_insample,num_iteration=lgbm_model.best_iteration)\n",
    "                    oot_pred = lgbm_model.predict(X_oot,num_iteration=lgbm_model.best_iteration)\n",
    "                    stop=datetime.now()\n",
    "                    print(\"Model Prediction start\")\n",
    "                    logloss=metrics.log_loss(oot_act, oot_pred)\n",
    "                    roc_auc_insample = roc_auc_score(insample_act, insample_pred)\n",
    "                    roc_auc_oot      = roc_auc_score(oot_act, oot_pred)\n",
    "                    gini_insample    = round((2 * roc_auc_insample) - 1, 3)\n",
    "                    gini_oot         = round((2 * roc_auc_oot) - 1, 3)\n",
    "                    print(\"Hyperparamter tuning start\")\n",
    "                    Hyperparameter_search_lightgbm = Hyperparameter_search_lightgbm.append(\n",
    "                        pd.Series({'subsample' :subsample,'max_depth' :max_depth,'learning Rate' : lr ,\n",
    "                                   'leaf_size' : num_leaves,'log_loss' : logloss, \n",
    "                                   'gini_insample' :gini_insample ,'gini_oot' :gini_oot}),ignore_index=True)\n",
    "                    print(Hyperparameter_search_lightgbm)\n",
    "\n",
    "Hyperparameter_search_lightgbm.to_csv('sphonic_ltet50k_model'+\"_\"+str(stop)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires graphviz and python-graphviz conda packages\n",
    "import graphviz\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"auc\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "xgb.plot_importance(xgb_model)\n",
    "\n",
    "# plot the output tree via matplotlib, specifying the ordinal number of the target tree\n",
    "# xgb.plot_tree(xgb_model, num_trees=xgb_model.best_iteration)\n",
    "\n",
    "# converts the target tree to a graphviz instance\n",
    "xgb.to_graphviz(xgb_model, num_trees=xgb_model.best_iteration)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful resources\n",
    "\n",
    "Notebooks:\n",
    "https://bluetigers-fcpmodels-notebook.notebook.eu-west-1.sagemaker.aws/notebooks/Fraud-Prevention-Model/Model_Risk_Team_Code/Sphonic_ltet50k_model.ipynb\n",
    "https://bluetigers-fcpmodels-notebook.notebook.eu-west-1.sagemaker.aws/notebooks/Fraud-Prevention-Model/Fraud%20Prevention%20-%20Pilot%20Model.ipynb\n",
    "https://bluetigers-fcpmodels-notebook.notebook.eu-west-1.sagemaker.aws/notebooks/Fraud-Prevention-Model/FCP%20model%20(Draft%20version).ipynb\n",
    "https://bluetigers-fcpmodels-notebook.notebook.eu-west-1.sagemaker.aws/notebooks/Fraud-Prevention-Model/Fraud%20Prevention%20-%20CIFAS_Triggerred.ipynb\n",
    "\n",
    "Kaggle:\n",
    "https://www.kaggle.com/stephaniestallworth/titanic-eda-classification-end-to-end\n",
    "https://www.kaggle.com/smmingg/past-loan-data-eda-and-classification\n",
    "https://www.kaggle.com/lucidlenn/data-analysis-and-classification-using-xgboost\n",
    "https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn\n",
    "https://www.kaggle.com/dansbecker/xgboost\n",
    "\n",
    "\n",
    "Hyperparameter tuning:\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "\n",
    "Categorical Data Encoding:\n",
    "https://www.pluralsight.com/guides/handling-categorical-data-in-machine-learning-models\n",
    "https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512\n",
    "https://www.datacamp.com/community/tutorials/categorical-data\n",
    "    \n",
    "\n",
    "Cross Validation using Resampling:\n",
    "https://www.kaggle.com/sasha18/resampling-methods-using-bootstrap-cv\n",
    "    \n",
    "\n",
    "Imbalanced Data Handling:\n",
    "https://www.kaggle.com/shahules/tackling-class-imbalance\n",
    "https://towardsdatascience.com/sampling-techniques-for-extremely-imbalanced-data-part-i-under-sampling-a8dbc3d8d6d8\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
